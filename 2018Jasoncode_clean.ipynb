{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWFEOD0jdenS7Xq2+7213G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KVincent007/Predicting-age-from-the-transcriptome-of-human-dermal-fibroblasts/blob/master/2018Jasoncode_clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "iicR8RfR561X",
        "outputId": "82764f92-d395-4389-8c3f-767e4428e140"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-1553505635.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# --- 3. Dimensionality Reduction ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mX_pca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# --- 4. Split Data ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \"\"\"\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_is_centered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mU\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# The copy will happen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# later, only if needed, once the solver negotiation below is done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
          ]
        }
      ],
      "source": [
        "# Transcriptomic Age Prediction (replication attempt of 2018 model by Jason)\n",
        "\n",
        "# --- 1. Setup ---\n",
        "!pip install scikit-learn pandas matplotlib seaborn --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import ElasticNetCV, LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import StratifiedKFold, RepeatedKFold\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# --- 2. Load and Prepare Data ---\n",
        "tpm = pd.read_csv(\"/content/merged_TPM_with_metadata_filtered (1).csv\", index_col=0)\n",
        "X = tpm.drop(columns=['Age', 'disease', 'sex'])\n",
        "y_reg = tpm['Age']\n",
        "\n",
        "# Binned age for classification\n",
        "y_cls = pd.cut(y_reg, bins=[0, 21, 41, 61, 81, 100], labels=['0-20', '21-40', '41-60', '61-80', '81-100'], include_lowest=True)\n",
        "\n",
        "# Stage 1: TPM >= 5 in at least 10% of samples\n",
        "mask_expr = (X >= 5).sum(axis=0) >= 0.1 * X.shape[0]\n",
        "X_filtered = X.loc[:, mask_expr]\n",
        "\n",
        "# Log-transform and scale TPMs\n",
        "X_log = np.log2(X_filtered + 1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_log)\n",
        "\n",
        "# --- 3. Dimensionality Reduction ---\n",
        "pca = PCA(n_components=100, random_state=42)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# --- 4. Split Data ---\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "  X_pca, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(\n",
        "  X_pca, y_cls, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 5. Helper Functions ---\n",
        "def train_regressor(model, X_train, y_train):\n",
        "  return model.fit(X_train, y_train)\n",
        "\n",
        "def evaluate_regressor(model, X_test, y_test, name):\n",
        "  y_pred = model.predict(X_test)\n",
        "  mae = mean_absolute_error(y_test, y_pred)\n",
        "  r2 = r2_score(y_test, y_pred)\n",
        "  print(f\"{name} → MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
        "  return y_pred\n",
        "\n",
        "def train_and_report_classifier(model, X_train, y_train, X_test, y_test, name):\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(f\"\\n{name} Classification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "  return y_pred\n",
        "\n",
        "# --- 6. Train and Evaluate Regression Models with Grid Search---\n",
        "##regressors = {\n",
        "##\"ElasticNet (L1+L2)\": ElasticNetCV(cv=10, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], random_state=42, n_jobs=-1),\n",
        "##\"Linear Regression\": LinearRegression(),\n",
        "##\"SVR (RBF + L2)\": SVR(kernel='rbf')\n",
        "##}\n",
        "\n",
        "##predictions = {}\n",
        "##for name, model in regressors.items():\n",
        "##  model.fit(X_train_reg, y_train_reg)\n",
        "##  predictions[name] = evaluate_regressor(model, X_test_reg, y_test_reg, name)\n",
        "\n",
        "# Define repeated cross-validation\n",
        "repeated_cv = RepeatedKFold(n_splits=10, n_repeats=2, random_state=42)\n",
        "\n",
        "# --- ElasticNet Grid Search ---\n",
        "enet = ElasticNet(max_iter=10000)\n",
        "enet_params = {\n",
        "    'alpha': np.logspace(-2, 1, 5),\n",
        "    'l1_ratio': [0.1, 0.5, 0.7, 0.9, 0.95, 1.0]\n",
        "}\n",
        "enet_grid = GridSearchCV(enet, enet_params, cv=repeated_cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "enet_grid.fit(X_pca, y_reg)\n",
        "print(\"Best ElasticNet parameters:\", enet_grid.best_params_)\n",
        "\n",
        "# --- SVR Grid Search (RBF + Poly) ---\n",
        "svr = SVR()\n",
        "svr_params = {\n",
        "    'kernel': ['rbf', 'poly'],\n",
        "    'C': [1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.1, 0.2],\n",
        "    'degree': [2, 3, 4]  # used only if kernel='poly'\n",
        "}\n",
        "svr_grid = GridSearchCV(svr, svr_params, cv=repeated_cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "svr_grid.fit(X_pca, y_reg)\n",
        "print(\"Best SVR parameters:\", svr_grid.best_params_)\n",
        "\n",
        "# Linear Regression (no tuning)\n",
        "lr_model = LinearRegression()\n",
        "lr_model.fit(X_train_reg, y_train_reg)\n",
        "\n",
        "# Evaluate all\n",
        "predictions = {}\n",
        "predictions[\"ElasticNet (GridSearch)\"] = evaluate_regressor(elasticnet_search.best_estimator_, X_test_reg, y_test_reg, \"ElasticNet (GridSearch)\")\n",
        "predictions[\"SVR (GridSearch)\"] = evaluate_regressor(svr_search.best_estimator_, X_test_reg, y_test_reg, \"SVR (GridSearch)\")\n",
        "predictions[\"Linear Regression\"] = evaluate_regressor(lr_model, X_test_reg, y_test_reg, \"Linear Regression\")\n",
        "\n",
        "# --- 7. LDA Ensemble with Shrinkage and CV-Based Gene Selection ---\n",
        "n_splits = 10\n",
        "n_repeats = 5\n",
        "top_k_genes = 500  # or any number based on prior performance tuning\n",
        "\n",
        "# store outputs\n",
        "all_true_cls = []\n",
        "all_pred_cls = []\n",
        "all_true_num = []\n",
        "all_pred_num = []\n",
        "\n",
        "# Mapping from bins to midpoint numeric age\n",
        "age_bin_to_mid = {\n",
        "    '0-20': 10,\n",
        "    '21-40': 30,\n",
        "    '41-60': 50,\n",
        "    '61-80': 70,\n",
        "    '81-100': 90\n",
        "}\n",
        "\n",
        "print(\"\\n🔁 Running LDA Ensemble with Shrinkage (5×10-fold)... + Regression Evaluation\")\n",
        "\n",
        "for repeat in range(n_repeats):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=repeat)\n",
        "    for train_idx, test_idx in skf.split(X_scaled, y_cls):\n",
        "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
        "        y_train, y_test = y_cls.iloc[train_idx], y_cls.iloc[test_idx]\n",
        "        y_test_num = y_reg.iloc[test_idx]\n",
        "\n",
        "        # Gene selection: top-k by ANOVA F-stat\n",
        "        selector = SelectKBest(score_func=f_classif, k=min(top_k_genes, X_train.shape[1]))\n",
        "        X_train_sel = selector.fit_transform(X_train, y_train)\n",
        "        X_test_sel = selector.transform(X_test)\n",
        "\n",
        "        # Shrinkage LDA\n",
        "        lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n",
        "        lda.fit(X_train_sel, y_train)\n",
        "        y_pred = lda.predict(X_test_sel)\n",
        "\n",
        "        # Store class labels\n",
        "        all_true_cls.extend(y_test)\n",
        "        all_pred_cls.extend(y_pred)\n",
        "\n",
        "        # Map predictions and ground truth to numeric\n",
        "        y_pred_num = [age_bin_to_mid[str(label)] for label in y_pred]\n",
        "        all_true_num.extend(y_test_num)\n",
        "        all_pred_num.extend(y_pred_num)\n",
        "\n",
        "# Final report\n",
        "print(\"\\n🧾 LDA Ensemble Classification Report:\")\n",
        "print(classification_report(all_true_cls, all_pred_cls))\n",
        "\n",
        "# Numeric (regression-style) metrics\n",
        "mae = mean_absolute_error(all_true_num, all_pred_num)\n",
        "r2 = r2_score(all_true_num, all_pred_num)\n",
        "print(f\"\\n📈 LDA Ensemble → MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
        "\n",
        "# --- 8. Visualization: Predicted vs Actual (Regression) ---\n",
        "plt.figure(figsize=(6, 6))\n",
        "for name, y_pred in predictions.items():\n",
        "  sns.scatterplot(x=y_test_reg, y=y_pred, label=name, alpha=0.7)\n",
        "\n",
        "plt.plot([y_test_reg.min(), y_test_reg.max()], [y_test_reg.min(), y_test_reg.max()], 'k--')\n",
        "plt.xlabel(\"Actual Age\")\n",
        "plt.ylabel(\"Predicted Age\")\n",
        "plt.title(\"Predicted vs Actual Age\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- 9. PCA Variance Explained ---\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance by PCA Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.metrics import mean_absolute_error, r2_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/merged_TPM_with_metadata_filtered (1).csv', index_col=0)\n",
        "X_full = df.drop(columns=['Age', 'sex', 'disease'])\n",
        "y = df['Age']\n",
        "y_cls = pd.cut(y, bins=[0, 21, 41, 61, 81, 100], labels=['0-20', '21-40', '41-60', '61-80', '81-100'])\n",
        "\n",
        "# --- CV Setup ---\n",
        "rkf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
        "\n",
        "# --- Storage ---\n",
        "results = {\n",
        "    \"ElasticNet\": [], \"SVR\": [], \"Linear\": [],\n",
        "    \"LDA_mae\": [], \"LDA_r2\": []\n",
        "}\n",
        "true_ages = []\n",
        "pred_ages_lda = []\n",
        "\n",
        "# --- Age bin midpoints ---\n",
        "age_bin_to_mid = {'0-20': 10, '21-40': 30, '41-60': 50, '61-80': 70, '81-100': 90}\n",
        "\n",
        "# --- Cross-Validation Loop ---\n",
        "for fold, (train_idx, test_idx) in enumerate(rkf.split(X_full, y_cls), 1):\n",
        "    X_train_raw, X_test_raw = X_full.iloc[train_idx], X_full.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    y_cls_train = y_cls.iloc[train_idx]\n",
        "    y_test_cls = y_cls.iloc[test_idx]\n",
        "\n",
        "    # --- Fold-Specific Gene Filtering (original) ---\n",
        "    #gene_mask = []\n",
        "    #for gene in X_train_raw.columns:\n",
        "    #    gene_vals = X_train_raw[gene]\n",
        "    #    if gene_vals.max() > 5 and (gene_vals.max() / gene_vals.replace(0, np.nan).min()) >= 5:\n",
        "    #        gene_mask.append(gene)\n",
        "    #X_train = X_train_raw[gene_mask].copy()\n",
        "    #X_test = X_test_raw[gene_mask].copy()\n",
        "\n",
        "    gene_mask = []\n",
        "    for gene in X_train_raw.columns:\n",
        "        gene_vals = X_train_raw[gene]\n",
        "        if gene_vals.max() > 5:\n",
        "            nonzero = gene_vals[gene_vals > 0]\n",
        "            if not nonzero.empty and gene_vals.max() / nonzero.min() >= 5:\n",
        "                gene_mask.append(gene)\n",
        "\n",
        "    if not gene_mask:\n",
        "        raise ValueError(f\"No genes passed filtering in fold {fold}\")\n",
        "\n",
        "\n",
        "    # To ensure X-train and y-train always have the same number of rows\n",
        "    X_train = X_train_raw[gene_mask].copy()\n",
        "    X_test  = X_test_raw[ gene_mask].copy()\n",
        "\n",
        "    # Now the shapes will match exactly:\n",
        "    #   X_train.shape[1] == X_test.shape[1]  AND\n",
        "    #   X_train.shape[0] == len(y_train)\n",
        "\n",
        "    # --- Log-transform + Standardize ---\n",
        "    X_train = np.log2(X_train + 1)\n",
        "    X_test = np.log2(X_test + 1)\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # --- ElasticNet ---\n",
        "    enet_pub = ElasticNet(alpha=0.1, l1_ratio=0.0, random_state=42, max_iter=10000, tol=0.01)\n",
        "    enet_pub.fit(X_train_scaled, y_train)\n",
        "    enet_pred = enet_pub.predict(X_test_scaled)\n",
        "    results['ElasticNet'].append((mean_absolute_error(y_test, enet_pred), r2_score(y_test, enet_pred)))\n",
        "\n",
        "    # --- SVR ---\n",
        "    svr_pub = SVR(kernel='poly', degree=2, C=10, epsilon=0.05, gamma=0.0002)\n",
        "    svr_pub.fit(X_train_scaled, y_train)\n",
        "    svr_pred = svr_pub.predict(X_test_scaled)\n",
        "    results['SVR'].append((mean_absolute_error(y_test, svr_pred), r2_score(y_test, svr_pred)))\n",
        "\n",
        "    # --- Linear Regression ---\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train_scaled, y_train)\n",
        "    lr_pred = lr.predict(X_test_scaled)\n",
        "    results['Linear'].append((mean_absolute_error(y_test, lr_pred), r2_score(y_test, lr_pred)))\n",
        "\n",
        "    # --- LDA (classification) ---\n",
        "    lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n",
        "    lda.fit(X_train_scaled, y_cls_train)\n",
        "    y_pred_cls = lda.predict(X_test_scaled)\n",
        "    y_pred_num = [age_bin_to_mid[str(c)] for c in y_pred_cls]\n",
        "    true_ages.extend(y.iloc[test_idx])\n",
        "    pred_ages_lda.extend(y_pred_num)\n",
        "\n",
        "# --- Evaluation ---\n",
        "def summarize_model(name, metrics):\n",
        "    mae = np.mean([m[0] for m in metrics])\n",
        "    r2 = np.mean([m[1] for m in metrics])\n",
        "    print(f\"{name} → MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
        "\n",
        "print(\"\\n📊 Repeated CV Summary:\")\n",
        "summarize_model(\"ElasticNet\", results[\"ElasticNet\"])\n",
        "summarize_model(\"SVR\", results[\"SVR\"])\n",
        "summarize_model(\"Linear\", results[\"Linear\"])\n",
        "\n",
        "# LDA evaluation\n",
        "lda_mae = mean_absolute_error(true_ages, pred_ages_lda)\n",
        "lda_r2 = r2_score(true_ages, pred_ages_lda)\n",
        "print(f\"LDA → MAE: {lda_mae:.2f} | R²: {lda_r2:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVHtBeQtOLCf",
        "outputId": "815789ae-3f16-4b97-c354-e11a623ad9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Repeated CV Summary:\n",
            "ElasticNet → MAE: 12.78 | R²: 0.67\n",
            "SVR → MAE: 18.33 | R²: 0.38\n",
            "Linear → MAE: 11.85 | R²: 0.71\n",
            "LDA → MAE: 9.28 | R²: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RepeatedKFold, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Load Data ---\n",
        "df = pd.read_csv('/content/merged_TPM_with_metadata_filtered (1).csv', index_col=0)\n",
        "X_full = df.drop(columns=['Age', 'sex', 'disease'])\n",
        "y = df['Age']\n",
        "y_cls = pd.cut(y, bins=[0, 21, 41, 61, 81, 100],\n",
        "               labels=['0-20', '21-40', '41-60', '61-80', '81-100'],\n",
        "               include_lowest=True)\n",
        "\n",
        "# --- CV Setup ---\n",
        "outer_cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
        "\n",
        "# parameter grids for inner search\n",
        "enet_grid = {\n",
        "    'alpha': [0.01, 0.1, 1.0],\n",
        "    'l1_ratio': [0.0, 0.5, 1.0]\n",
        "}\n",
        "svr_grid = {\n",
        "    'kernel': ['poly', 'rbf'],\n",
        "    'degree': [2, 3],             # only for poly\n",
        "    'C': [1, 10],\n",
        "    'epsilon': [0.05, 0.1],\n",
        "    'gamma': ['scale', 0.0002]\n",
        "}\n",
        "\n",
        "results = {'ElasticNet': [], 'SVR': [], 'Linear': [], 'LDA': []}\n",
        "true_ages, pred_ages_lda = [], []\n",
        "age_bin_to_mid = {'0-20':10,'21-40':30,'41-60':50,'61-80':70,'81-100':90}\n",
        "\n",
        "for train_idx, test_idx in outer_cv.split(X_full, y_cls):\n",
        "    X_train_raw, X_test_raw = X_full.iloc[train_idx], X_full.iloc[test_idx]\n",
        "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "    y_cls_train = y_cls.iloc[train_idx]\n",
        "\n",
        "    # your fold-specific gene filtering\n",
        "    mask = []\n",
        "    for g in X_train_raw.columns:\n",
        "        vals = X_train_raw[g]\n",
        "        if vals.max()>5 and (vals.max()/vals.replace(0,np.nan).min())>=5:\n",
        "            mask.append(g)\n",
        "    X_train = np.log2(X_train_raw[mask] + 1)\n",
        "    X_test  = np.log2(X_test_raw[mask]  + 1)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_s = scaler.fit_transform(X_train)\n",
        "    X_test_s  = scaler.transform(X_test)\n",
        "\n",
        "    # --- ElasticNet with inner GridSearchCV ---\n",
        "    inner_cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
        "    enet = GridSearchCV(ElasticNet(max_iter=5000), enet_grid,\n",
        "                        cv=inner_cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    enet.fit(X_train_s, y_train)\n",
        "    pred = enet.best_estimator_.predict(X_test_s)\n",
        "    results['ElasticNet'].append((mean_absolute_error(y_test, pred),\n",
        "                                  r2_score(y_test, pred)))\n",
        "\n",
        "    # --- SVR with inner GridSearchCV ---\n",
        "    svr = GridSearchCV(SVR(), svr_grid,\n",
        "                       cv=inner_cv, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    svr.fit(X_train_s, y_train)\n",
        "    pred = svr.best_estimator_.predict(X_test_s)\n",
        "    results['SVR'].append((mean_absolute_error(y_test, pred),\n",
        "                           r2_score(y_test, pred)))\n",
        "\n",
        "    # --- Linear Regression (no tuning) ---\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train_s, y_train)\n",
        "    pred = lr.predict(X_test_s)\n",
        "    results['Linear'].append((mean_absolute_error(y_test, pred),\n",
        "                              r2_score(y_test, pred)))\n",
        "\n",
        "    # --- LDA (no tuning) ---\n",
        "    lda = LinearDiscriminantAnalysis(solver='eigen', shrinkage='auto')\n",
        "    lda.fit(X_train_s, y_cls_train)\n",
        "    cls_pred = lda.predict(X_test_s)\n",
        "    true_ages.extend(y_test)\n",
        "    pred_ages_lda.extend([age_bin_to_mid[str(c)] for c in cls_pred])\n",
        "\n",
        "# --- Summarize ---\n",
        "def summarize(name, vals):\n",
        "    mae = np.mean([v[0] for v in vals]); r2 = np.mean([v[1] for v in vals])\n",
        "    print(f\"{name} → MAE: {mae:.2f} | R²: {r2:.2f}\")\n",
        "\n",
        "print(\"\\nRegression Models:\")\n",
        "summarize(\"ElasticNet\", results['ElasticNet'])\n",
        "summarize(\"SVR\", results['SVR'])\n",
        "summarize(\"LinearRegression\", results['Linear'])\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "print(\"\\nLDA →\", end=\" \")\n",
        "print(f\"MAE: {mean_absolute_error(true_ages, pred_ages_lda):.2f} | \"\n",
        "      f\"R²: {r2_score(true_ages, pred_ages_lda):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbQ0G7OGEn6B",
        "outputId": "e1ee4a4e-23cc-44b0-d7bc-42c743b66d33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Regression Models:\n",
            "ElasticNet → MAE: 12.09 | R²: 0.69\n",
            "SVR → MAE: 17.84 | R²: 0.40\n",
            "LinearRegression → MAE: 11.85 | R²: 0.71\n",
            "\n",
            "LDA → MAE: 9.28 | R²: 0.73\n"
          ]
        }
      ]
    }
  ]
}